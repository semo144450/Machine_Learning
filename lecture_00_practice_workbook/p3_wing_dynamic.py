# äºŒç»´ç¿¼å‹æ°”åŠ¨åŠ›é¢„æµ‹

# 1. å¯¼å…¥ç›¸å…³åº“
import pandas as pd
import time
import os
from sklearn.preprocessing import MinMaxScaler
from keras import Sequential, layers, optimizers

# 2. æ•°æ®é¢„å¤„ç†

file_path = os.path.dirname(__file__)

# è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†åˆ’åˆ†
sample_x1 = pd.read_csv(file_path + '/3_airfoil/sample_x.csv', on_bad_lines='skip') # è¯»å–æ ·æœ¬è¾“å…¥ 100*2
sample_y1 = pd.read_csv(file_path + '/3_airfoil/sample_y.csv', on_bad_lines='skip') # è¯»å–æ ·æœ¬è¾“å‡º 100*3
sample_x = sample_x1.values
sample_y = sample_y1.values

n_tr_val = 80 #ç”¨äºè®­ç»ƒå’ŒéªŒè¯çš„æ ·æœ¬æ•°é‡

# è®­ç»ƒé›†å’ŒéªŒè¯é›†è¾“å…¥ï¼Œç¬¬1å’Œ2åˆ—åˆ†åˆ«ä¸ºæ¥æµé€Ÿåº¦ ğ‘‰âˆ å’Œæ”»è§’ ğ›¼ çš„å–å€¼
train_x = sample_x[0:n_tr_val,:]
# è®­ç»ƒé›†å’ŒéªŒè¯é›†è¾“å‡ºï¼Œç¬¬1è‡³3åˆ—åˆ†åˆ«ä¸ºä¸è¾“å…¥æ¥æµé€Ÿåº¦å’Œæ”»è§’å¯¹åº”çš„CFDè®¡ç®—å¾—åˆ°çš„å‡åŠ›ã€é˜»åŠ›å’ŒåŠ›çŸ©
train_y = sample_y[0:n_tr_val,:]

test_x = sample_x[n_tr_val:,:] # æµ‹è¯•é›†è¾“å…¥
test_y = sample_y[n_tr_val:,:] # æµ‹è¯•é›†è¾“å‡º
feature = 3 # è¾“å‡ºç‰¹å¾æ•°é‡

# è¾“å…¥å’Œè¾“å‡ºæ•°æ®çš„æ ‡å‡†åŒ–å¤„ç†

# æ•°æ®ç¼©æ”¾åˆ° [-1, 1] èŒƒå›´
scalerX = MinMaxScaler(feature_range=(-1,1))
sample_x_scaler = scalerX.fit_transform(sample_x)
scalerY = MinMaxScaler(feature_range=(0,1))
sample_y_scaler = scalerY.fit_transform(sample_y)

train_x_scaler = sample_x_scaler[0:n_tr_val,:]
train_y_scaler = sample_y_scaler[0:n_tr_val,:]
test_x_scaler = sample_x_scaler[n_tr_val:,:]
test_y_scaler = sample_y_scaler[n_tr_val:,:]

# 3. ç¥ç»ç½‘ç»œ

# æ¨¡å‹å®šä¹‰
# éšè—å±‚çš„ç¥ç»å…ƒæ•°ç›®ä¸º5ï¼Œè¾“å…¥å±‚çš„ç»´æ•°ä¸º2ï¼Œè¾“å‡ºå±‚çš„ç»´æ•°ä¸º3
model = Sequential()

# éšè—å±‚å’Œè¾“å‡ºå±‚ä¸­çš„ç¥ç»å…ƒåˆ†åˆ«ä½¿ç”¨sigmoidå‡½æ•°å’Œreluå‡½æ•°ä½œä¸ºæ¿€æ´»å‡½æ•°
# æ¯ä¸ªç¥ç»å…ƒçš„æƒé‡é‡‡ç”¨normalå‡†åˆ™è¿›è¡Œåˆå§‹åŒ–ï¼Œå³åˆå§‹åŒ–ä¸ºä¸€ç»„æ»¡è¶³å‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 0.05 çš„é«˜æ–¯åˆ†å¸ƒçš„éšæœºæ•°
model.add(layers.Dense(units=5,input_dim=2,kernel_initializer='normal',activation='sigmoid'))
model.add(layers.Dense(units=feature,kernel_initializer='normal',activation='relu'))

# è¾“å‡ºæ¨¡å‹å‚æ•°ä¿¡æ¯
model.summary()

# æ¨¡å‹ç¼–è¯‘
adam = optimizers.Adam(learning_rate=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-7, decay=0, amsgrad=False)

# é€‰æ‹©è¯¯å·®è¯„ä»·å‡†åˆ™å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€å‚æ•°ä¼˜åŒ–æ–¹æ³•
model.compile(loss='MSE',optimizer='adam')

# æ¨¡å‹è®­ç»ƒ
time_start = time.time()
history = model.fit(train_x_scaler, train_y_scaler, epochs = 200, batch_size = 8, validation_split = 0.25, verbose = 2, shuffle = False)
time_end = time.time()
print('totally cost',time_end-time_start)
loss_y = history.history['loss'] # æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„å‡æ–¹è¯¯å·®å†ç¨‹
val_loss_y = history.history['val_loss'] # æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„å‡æ–¹è¯¯å·®å†ç¨‹

# 4. æ¨¡å‹é¢„æµ‹
train_y_scaler_pre = model.predict(train_x_scaler) # æ¨¡å‹åœ¨è®­ç»ƒå’ŒéªŒè¯é›†ä¸Šçš„æ ‡å‡†åŒ–é¢„æµ‹å€¼
train_y_pre = scalerY.inverse_transform(train_y_scaler_pre) # æ¨¡å‹åœ¨è®­ç»ƒå’ŒéªŒè¯é›†ä¸Šçš„é¢„æµ‹å€¼
test_y_scaler_pre = model.predict(test_x_scaler) # æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ ‡å‡†åŒ–é¢„æµ‹å€¼
test_y_pre = scalerY.inverse_transform(test_y_scaler_pre) # æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹å€¼

# æ°”åŠ¨åŠ›é¢„æµ‹å€¼å’Œæ¨¡å‹è®­ç»ƒè¯¯å·®å†ç¨‹è¾“å‡º

name1 = ['lift','drag','moment']
ex1 = pd.DataFrame(columns = name1, data = train_y_pre)
ex1.to_csv(file_path + '/3_airfoil/p3_train_y_pre.csv') # è¾“å‡ºæ¨¡å‹åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šçš„é¢„æµ‹å€¼

name2 = ['lift','drag','moment']
ex2 = pd.DataFrame(columns = name2, data = test_y_pre)
ex2.to_csv(file_path + '/3_airfoil/p3_test_y_pre.csv') # è¾“å‡ºæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹å€¼

name3 = ['loss']
ex3 = pd.DataFrame(columns = name3, data = loss_y)
ex3.to_csv(file_path + '/3_airfoil/p3_loss.csv') # è¾“å‡ºæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„å‡æ–¹è¯¯å·®å†ç¨‹

name4 = ['val_loss']
ex4 = pd.DataFrame(columns = name4, data = val_loss_y)
ex4.to_csv(file_path + '/3_airfoil/p3_val_loss.csv') # è¾“å‡ºæ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„å‡æ–¹è¯¯å·®å†ç¨‹